{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw23sCi-Yp1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, Input, Flatten, Reshape\n",
        "from keras.models import Model,Sequential\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "epochs = 10000\n",
        "batch_size = 32\n",
        "img_shape = (28,28,1)\n",
        "d_out = []\n",
        "g_out = []\n",
        "\n",
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
        "    \n",
        "    # convert shape of x_train from (60000, 28, 28) to (60000, 784) \n",
        "    # 784 columns per row\n",
        "    x_train = x_train.reshape(60000, 784)\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "def adam_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "def create_generator():\n",
        "    gmodel = Sequential()\n",
        "    gmodel.add(Dense(256, input_dim=100))\n",
        "    gmodel.add(LeakyReLU(alpha=0.2))\n",
        "    gmodel.add(BatchNormalization(momentum=0.8))\n",
        "    gmodel.add(Dense(512))\n",
        "    gmodel.add(LeakyReLU(alpha=0.2))\n",
        "    gmodel.add(BatchNormalization(momentum=0.8))\n",
        "    gmodel.add(Dense(1024))\n",
        "    gmodel.add(LeakyReLU(alpha=0.2))\n",
        "    gmodel.add(BatchNormalization(momentum=0.8))\n",
        "    gmodel.add(Dense(units=784, activation='tanh'))\n",
        "    return gmodel\n",
        "\n",
        "def create_discriminator():\n",
        "    dmodel = Sequential()\n",
        "    dmodel.add(Dense(512,input_dim=784))\n",
        "    dmodel.add(LeakyReLU(alpha=0.2))\n",
        "    dmodel.add(Dense(256))\n",
        "    dmodel.add(LeakyReLU(alpha=0.2))\n",
        "    dmodel.add(Dense(1, activation='sigmoid'))\n",
        "    return dmodel \n",
        "\n",
        "\n",
        "def create_gan(discriminator, generator):\n",
        "    discriminator.trainable=False\n",
        "    gan_input = Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan= Model(inputs=gan_input, outputs=gan_output)\n",
        "    return gan\n",
        "\n",
        "\n",
        "def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(10,10)):\n",
        "    noise= np.random.normal(loc=0, scale=1, size=[examples, 100])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(100,28,28)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"images/%d.png\"  %epoch)\n",
        "  \n",
        "def training(epochs,batch_size):    \n",
        "    (X_train, y_train,X_test, y_test)=load_data()\n",
        "    print(X_train.shape)\n",
        "    X_test = X_test.reshape(10000,784)\n",
        "    print(X_test.shape,y_test.shape)\n",
        "    generator= create_generator()\n",
        "    discriminator= create_discriminator()\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer(),metrics=['accuracy'])   \n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer(),metrics=['accuracy'])   \n",
        " \n",
        "    gan = create_gan(discriminator, generator)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator.summary()\n",
        "    generator.summary()\n",
        "    gan.summary()\n",
        "    \n",
        "    for e in range(1,epochs+1 ):\n",
        "            print(\"Epoch %d\" %e)\n",
        "            for _ in tqdm(range(batch_size)):\n",
        "            #generate  random noise as an input  to  initialize the  generator\n",
        "                noise= np.random.normal(0,1, [batch_size, 100])\n",
        "                \n",
        "                # Generate fake MNIST images from noised input\n",
        "                generated_images = generator.predict(noise)\n",
        "                \n",
        "                # Get a random set of  real images\n",
        "                image_batch = X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n",
        "                #Construct different batches of  real and fake data \n",
        "                X= np.concatenate([image_batch, generated_images])\n",
        "                \n",
        "                # Labels for generated and real data\n",
        "                y_dis=np.zeros(2*batch_size)\n",
        "                y_dis[:batch_size]=0.9\n",
        "                \n",
        "                #Pre train discriminator on  fake and real data  before starting the gan. \n",
        "                discriminator.trainable=True\n",
        "               \n",
        "                #d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "                d_loss = discriminator.fit(X, y_dis)\n",
        "                print(\"Discriminator LOSS\",d_loss)\n",
        "                d_out.append(np.array(d_loss))\n",
        "                \n",
        "                # Fixing Discriminator weights\n",
        "                discriminator.trainable=False\n",
        "                  #Tricking the noised input of the Generator as real data\n",
        "                noise= np.random.normal(0,1, [batch_size, 100])\n",
        "                y_gen = np.ones(batch_size)\n",
        "           \n",
        "                #Train the GAN     \n",
        "                g_loss= gan.fit(noise,y_gen)\n",
        "                \n",
        "            if e == 1 or e % 20 == 0:\n",
        "                plot_generated_images(e,generator)\n",
        "               \n",
        "    generator.model.save(\"g_images_model.h5\")\n",
        "    discriminator.model.save(\"d_images_model.h5\")\n",
        "    discriminator.trainable = False\n",
        "    \n",
        "    #Evaluate the Generator\n",
        "    scores = discriminator.evaluate(X_test,y_test,batch_size=32)\n",
        "    \n",
        "    print(\"%s: %.2f%%\" % (discriminator.metrics_names[1], scores[1]*100))\n",
        "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(scores), np.std(scores)))           \n",
        "    print(\"%s: %.2f%%\" % (discriminator.metrics_names[0], scores[0]*100))          \n",
        "  \n",
        "    \n",
        "training(epochs,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}